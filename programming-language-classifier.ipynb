{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all the things! sklearn and glob namely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.datasets import load_files\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the data from the 'data' folder. load_files takes the folder name as the target value for the data in each file in that folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = load_files('data', encoding='UTF-8', decode_error='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vectorizer to transform the text in the files into token counts and train it for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 2))\n",
    "X_train_counts = vec.fit_transform(dataset.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The tfidf transformer uses token frequency instead of straight counts. tfidf takes the length of the file into account when counting how many times a certain character, or set of characters (depending on how the vectorizer was set - in our case it takes up to 2 words as a token) appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The SGDClassifier is similar to the Naive Bayes in that it is what predicts new data fed into it. Scikit-learn's documentation describes it as \"a linear support vector machine (SVM), which is widely regarded as one of the best text classification algorithms (although it’s also a bit slower than naïve Bayes).\" It did indeed surpass the Naive Bayes by 5-10% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classifier Score: 0.9866496401207336\n"
     ]
    }
   ],
   "source": [
    "sgd = SGDClassifier(alpha=1e-3, random_state=42).fit(X_train_tfidf, dataset.target)\n",
    "print(\"Train classifier Score: {}\".format(sgd.score(X_train_tfidf, dataset.target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing a few funtions to read in test data for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file(code):\n",
    "    f = open(code)\n",
    "    text = [f.read()]\n",
    "    return text\n",
    "\n",
    "def get_lang(code_file):\n",
    "    test_doc = get_file(code_file)\n",
    "    X_new_counts = vec.transform(test_doc)\n",
    "    X_new_tfidf = tf_transformer.transform(X_new_counts)\n",
    "\n",
    "    predicted = dataset.target_names[sgd.predict(X_new_tfidf)[0]]\n",
    "\n",
    "    return predicted\n",
    "\n",
    "def run_tests(tests):\n",
    "    results = []\n",
    "    for t in tests:\n",
    "        results.append(get_lang(t))\n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests is a list of the test files in the test folder. Results is the output of running those tests through the classifier. Test_expected holds the actual code type of each test to check the results of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tests = glob.glob('test/*')\n",
    "results = run_tests(tests)\n",
    "test_expected = [('1', 'clojure'), ('10', 'javascript'), ('11', 'javascript'), ('12', 'javascript'), ('13', 'ruby'),\n",
    "                 ('14', 'ruby'), ('15', 'ruby'), ('16', 'haskell'), ('17', 'haskell'), ('18', 'haskell'),\n",
    "                 ('19', 'scheme'), ('2', 'clojure'), ('20', 'scheme'), ('21', 'scheme'), ('22', 'java'),\n",
    "                 ('23', 'java'), ('24', 'scala'), ('25', 'scala'), ('26', 'tcl'), ('27', 'tcl'), ('28', 'php'),\n",
    "                 ('29', 'php'), ('3', 'clojure'), ('30', 'php'), ('31', 'ocaml'), ('32', 'ocaml'), ('4', 'clojure'),\n",
    "                 ('5', 'python'), ('6', 'python'), ('7', 'python'), ('8', 'python'), ('9', 'javascript')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Puts the results and target together and compares them. Outputs print statements for each test* and calculates the percent it got right at the bottom as well as noting which languages it failed to recognize correctly.\n",
    "### *tcl files were not included in training, and so have been omitted in the testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Expected: clojure; Predicted: clojure; CORRECT\n",
      "Test 10: Expected: javascript; Predicted: javascript; CORRECT\n",
      "Test 11: Expected: javascript; Predicted: javascript; CORRECT\n",
      "Test 12: Expected: javascript; Predicted: javascript; CORRECT\n",
      "Test 13: Expected: ruby; Predicted: ruby; CORRECT\n",
      "Test 14: Expected: ruby; Predicted: ruby; CORRECT\n",
      "Test 15: Expected: ruby; Predicted: ruby; CORRECT\n",
      "Test 16: Expected: haskell; Predicted: haskell; CORRECT\n",
      "Test 17: Expected: haskell; Predicted: haskell; CORRECT\n",
      "Test 18: Expected: haskell; Predicted: haskell; CORRECT\n",
      "Test 19: Expected: scheme; Predicted: scheme; CORRECT\n",
      "Test 2: Expected: clojure; Predicted: clojure; CORRECT\n",
      "Test 20: Expected: scheme; Predicted: scheme; CORRECT\n",
      "Test 21: Expected: scheme; Predicted: scheme; CORRECT\n",
      "Test 22: Expected: java; Predicted: java; CORRECT\n",
      "Test 23: Expected: java; Predicted: java; CORRECT\n",
      "Test 24: Expected: scala; Predicted: scala; CORRECT\n",
      "Test 25: Expected: scala; Predicted: scala; CORRECT\n",
      "Test 28: Expected: php; Predicted: php; CORRECT\n",
      "Test 29: Expected: php; Predicted: php; CORRECT\n",
      "Test 3: Expected: clojure; Predicted: clojure; CORRECT\n",
      "Test 30: Expected: php; Predicted: php; CORRECT\n",
      "Test 31: Expected: ocaml; Predicted: ocaml; CORRECT\n",
      "Test 32: Expected: ocaml; Predicted: ocaml; CORRECT\n",
      "Test 4: Expected: clojure; Predicted: clojure; CORRECT\n",
      "Test 5: Expected: python; Predicted: python; CORRECT\n",
      "Test 6: Expected: python; Predicted: python; CORRECT\n",
      "Test 7: Expected: python; Predicted: python; CORRECT\n",
      "Test 8: Expected: python; Predicted: python; CORRECT\n",
      "Test 9: Expected: javascript; Predicted: javascript; CORRECT\n",
      "\n",
      "Percent correct: %100.0\n",
      "Failed on:  []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "both = zip(test_expected, results)\n",
    "count = 0\n",
    "failed_on = []\n",
    "\n",
    "for i in both:\n",
    "    correct = 'WRONG'\n",
    "    if i[0][1] == i[1]:\n",
    "        correct = 'CORRECT'\n",
    "        count += 1\n",
    "        print('Test {}: Expected: {}; Predicted: {}; {}'.format(i[0][0],\n",
    "                                                                i[0][1], i[1],\n",
    "                                                                correct))\n",
    "    else:\n",
    "        if i[0][1] != 'tcl':\n",
    "            failed_on.append(i[0][1])\n",
    "            print('Test {}: Expected: {}; Predicted: {}; {}'.format(i[0][0],\n",
    "                                                                    i[0][1],\n",
    "                                                                    i[1],\n",
    "                                                                    correct))\n",
    "\n",
    "print('\\nPercent correct: %{}'.format(count/30*100))\n",
    "print('Failed on: ', failed_on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
